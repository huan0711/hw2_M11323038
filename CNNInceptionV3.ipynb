{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dde45e0-ce3d-4245-9f6b-d8fcbe49198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import tensorflow  as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,UpSampling2D,InputLayer,Reshape\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from keras.layers import Dropout,Activation,BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.utils import array_to_img\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, GlobalAveragePooling2D\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.utils import Bunch\n",
    "import math\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee1b247-5f16-475f-843b-d9a31ccadc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定圖片大小和路徑\n",
    "img_width, img_height = 299, 299  # InceptionV3的標準輸入大小\n",
    "train_data_dir = \"C:/Users/user/Desktop/MLwork2/train\"\n",
    "test_data_dir = \"C:/Users/user/Desktop/MLwork2/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70eafecf-4b80-41a7-866c-3da0945d6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [20, 40, 60]\n",
    "batch_size_list = [8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32abe247-a115-4002-b821-bbab84ae4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'C:/Users/user/Desktop/MLWORK/InceptionV3'\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d76297d-d8ab-4f40-bad0-eeb053692b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建CSV檔案記錄結果\n",
    "def create_result_csv_files():\n",
    "    csv_files = {}\n",
    "    \n",
    "    # 創建基本模型結果CSV\n",
    "    base_csv_path = os.path.join(results_dir, \"InceptionV3_base_model_results.csv\")\n",
    "    with open(base_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Model', 'Epochs', 'Train Accuracy', 'Train Loss', 'Val Accuracy', 'Val Loss', 'Test Accuracy', 'Test Loss'])\n",
    "    csv_files['base'] = base_csv_path\n",
    "    \n",
    "    # 創建微調模型結果CSV\n",
    "    tuned_csv_path = os.path.join(results_dir, \"InceptionV3_fine_tuned_model_results.csv\")\n",
    "    with open(tuned_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Model', 'Epochs', 'Train Accuracy', 'Train Loss', 'Val Accuracy', 'Val Loss', 'Test Accuracy', 'Test Loss'])\n",
    "    csv_files['tuned'] = tuned_csv_path\n",
    "    \n",
    "    # 創建模型比較CSV\n",
    "    comparison_csv_path = os.path.join(results_dir, \"InceptionV3_tuned_or_not_comparison.csv\")\n",
    "    with open(comparison_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Model', 'Epochs', 'Base Accuracy', 'Base Loss', 'Tuned Accuracy', 'Tuned Loss', 'Improvement'])\n",
    "    csv_files['comparison'] = comparison_csv_path\n",
    "    \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fa8356-58b2-417c-81bd-f52dbc5f3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料生成器 (無資料增強)\n",
    "def create_data_generators(batch_size):\n",
    "    # 僅做標準化處理，不增強數據\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255,\n",
    "        validation_split=0.2  # 保留驗證分割\n",
    "    )\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    val_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "198cf5c9-3ee0-4818-b22e-a849b2156475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建InceptionV3模型\n",
    "def create_inception_model():\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "    \n",
    "    # 凍結基礎模型的層\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # 添加新的分類層\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # InceptionV3通常使用GlobalAveragePooling而非Flatten\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  # 二元分類\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b67457e-0938-4d1d-9d8d-d536d5b6b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微調模型：解凍部分基礎模型的層\n",
    "def fine_tune_model(model):\n",
    "    # 計算要解凍的層數（所有模型使用相同的比例：頂部20%的層）\n",
    "    # 確保 model 是一個 Sequential 模型且第一個層是 base_model\n",
    "    if isinstance(model, Sequential) and len(model.layers) > 0:\n",
    "        base_model = model.layers[0] # <-- 正確地獲取 InceptionV3 基礎模型\n",
    "    else:\n",
    "        print(\"Warning: model structure unexpected, could not identify base_model for fine-tuning.\")\n",
    "        return model\n",
    "\n",
    "    if not hasattr(base_model, 'layers'):\n",
    "        print(\"Warning: base_model does not have layers to unfreeze.\")\n",
    "        return model\n",
    "\n",
    "    total_layers = len(base_model.layers) # <-- 計算 InceptionV3 基礎模型的層數\n",
    "    if total_layers == 0:\n",
    "         print(\"Warning: base_model has no layers to unfreeze.\")\n",
    "         return model\n",
    "\n",
    "    unfreeze_layers = max(0, int(total_layers * 0.2))  # 解凍頂部20%的層\n",
    "\n",
    "    # 先凍結所有基礎模型的層\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 然後解凍 InceptionV3 基礎模型頂部的指定層數\n",
    "    for layer in base_model.layers[-unfreeze_layers:]: # <-- 解凍 InceptionV3 基礎模型的頂部層\n",
    "        layer.trainable = True\n",
    "\n",
    "    print(f\"Fine-tuning model: Unfrozen {unfreeze_layers} layers out of {total_layers} total layers in the base model.\")\n",
    "\n",
    "    # 重新編譯模型以應用更低的學習率\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-5),  # 更低的學習率用於微調\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5827a7bc-c1b0-4d95-b952-ae498eb2d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練和評估模型\n",
    "def train_and_evaluate_model(model, train_generator, val_generator, test_generator, \n",
    "                           model_name, epochs, batch_size, is_fine_tuned=False):\n",
    "    model_prefix = \"fine_tuned_\" if is_fine_tuned else \"base_\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "        ModelCheckpoint(\n",
    "            os.path.join(results_dir, f\"{model_prefix}{model_name}_e{epochs}.h5\"),\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 訓練模型\n",
    "    print(f\"Training {'fine-tuned' if is_fine_tuned else 'base'} {model_name} with epochs={epochs}, batch_size={batch_size}\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_generator.samples // batch_size,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # 評估訓練集\n",
    "    train_eval = model.evaluate(train_generator)\n",
    "    train_loss, train_acc = train_eval\n",
    "    \n",
    "    # 評估驗證集\n",
    "    val_eval = model.evaluate(val_generator)\n",
    "    val_loss, val_acc = val_eval\n",
    "    \n",
    "    # 評估測試集\n",
    "    test_eval = model.evaluate(test_generator)\n",
    "    test_loss, test_acc = test_eval\n",
    "    \n",
    "    # 繪製訓練歷史\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 準確率圖\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], 'b', label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], 'r', label='Validation Accuracy')\n",
    "    plt.title(f'{model_prefix.capitalize()} {model_name} - Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 損失函數圖\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], 'b', label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], 'r', label='Validation Loss')\n",
    "    plt.title(f'{model_prefix.capitalize()} {model_name} - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f\"{model_prefix.lower()}_{model_name.lower()}_e{epochs}_b{batch_size}_history.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'train_accuracy': train_acc,\n",
    "        'train_loss': train_loss,\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d10ab06-06f1-4a4c-a6c7-218a69db1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將結果保存到CSV\n",
    "def save_results_to_csv(results, csv_path, model_name, epochs, is_fine_tuned=False):\n",
    "    with open(csv_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            model_name,\n",
    "            epochs,\n",
    "            results['train_accuracy'],\n",
    "            results['train_loss'],\n",
    "            results['val_accuracy'],\n",
    "            results['val_loss'],\n",
    "            results['test_accuracy'],\n",
    "            results['test_loss']\n",
    "        ])\n",
    "\n",
    "# 保存比較結果到CSV\n",
    "def save_comparison_to_csv(base_results, tuned_results, csv_path, model_name, epochs):\n",
    "    improvement = tuned_results['test_accuracy'] - base_results['test_accuracy']\n",
    "    \n",
    "    with open(csv_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            model_name,\n",
    "            epochs,\n",
    "            base_results['test_accuracy'],\n",
    "            base_results['test_loss'],\n",
    "            tuned_results['test_accuracy'],\n",
    "            tuned_results['test_loss'],\n",
    "            improvement\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02ad10c-7f23-4b5e-9235-c180e7c54c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Stage 1: Training and Evaluating Base Models ========\n",
      "\n",
      "===== Training Base InceptionV3 - Epochs: 20, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base InceptionV3 with epochs=20, batch_size=8\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 25s 613ms/step - loss: 0.7742 - accuracy: 0.4545 - val_loss: 0.6303 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 12s 540ms/step - loss: 0.5624 - accuracy: 0.6875 - val_loss: 0.5850 - val_accuracy: 0.7000\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 12s 536ms/step - loss: 0.5143 - accuracy: 0.7557 - val_loss: 0.5705 - val_accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 12s 523ms/step - loss: 0.4299 - accuracy: 0.8295 - val_loss: 0.5195 - val_accuracy: 0.8250\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 11s 507ms/step - loss: 0.3981 - accuracy: 0.8523 - val_loss: 0.5639 - val_accuracy: 0.7000\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 11s 504ms/step - loss: 0.3680 - accuracy: 0.8750 - val_loss: 0.5203 - val_accuracy: 0.7500\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 11s 508ms/step - loss: 0.3388 - accuracy: 0.8750 - val_loss: 0.5679 - val_accuracy: 0.6750\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.3692 - accuracy: 0.9091\n",
      "6/6 [==============================] - 3s 579ms/step - loss: 0.5433 - accuracy: 0.7727\n",
      "10/10 [==============================] - 5s 472ms/step - loss: 0.5713 - accuracy: 0.6875\n",
      "\n",
      "===== Training Base InceptionV3 - Epochs: 20, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base InceptionV3 with epochs=20, batch_size=16\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - 19s 1s/step - loss: 0.7810 - accuracy: 0.4886 - val_loss: 0.7346 - val_accuracy: 0.4062\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.6784 - accuracy: 0.5852 - val_loss: 0.6362 - val_accuracy: 0.6562\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.5615 - accuracy: 0.6648 - val_loss: 0.6095 - val_accuracy: 0.6875\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 11s 980ms/step - loss: 0.5070 - accuracy: 0.7443 - val_loss: 0.6465 - val_accuracy: 0.6562\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4835 - accuracy: 0.7614 - val_loss: 0.6448 - val_accuracy: 0.6562\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4367 - accuracy: 0.7841 - val_loss: 0.5744 - val_accuracy: 0.6562\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3896 - accuracy: 0.8693 - val_loss: 0.6243 - val_accuracy: 0.6250\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.3717 - accuracy: 0.8807 - val_loss: 0.5966 - val_accuracy: 0.6562\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3459 - accuracy: 0.8750 - val_loss: 0.5670 - val_accuracy: 0.6562\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.3344 - accuracy: 0.8750 - val_loss: 0.5328 - val_accuracy: 0.7188\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3374 - accuracy: 0.8864 - val_loss: 0.5542 - val_accuracy: 0.6875\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2754 - accuracy: 0.9091 - val_loss: 0.5507 - val_accuracy: 0.6875\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.2568 - accuracy: 0.9261 - val_loss: 0.4985 - val_accuracy: 0.6875\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.2623 - accuracy: 0.9545 - val_loss: 0.4766 - val_accuracy: 0.7500\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 11s 976ms/step - loss: 0.2539 - accuracy: 0.9091 - val_loss: 0.5693 - val_accuracy: 0.6875\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 11s 992ms/step - loss: 0.2393 - accuracy: 0.9432 - val_loss: 0.6219 - val_accuracy: 0.6250\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2239 - accuracy: 0.9205 - val_loss: 0.5632 - val_accuracy: 0.6250\n",
      "11/11 [==============================] - 9s 763ms/step - loss: 0.2242 - accuracy: 0.9545\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.5554 - accuracy: 0.7045\n",
      "5/5 [==============================] - 5s 970ms/step - loss: 0.4968 - accuracy: 0.7500\n",
      "\n",
      "===== Training Base InceptionV3 - Epochs: 40, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base InceptionV3 with epochs=40, batch_size=8\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 18s 620ms/step - loss: 0.7033 - accuracy: 0.5455 - val_loss: 0.6598 - val_accuracy: 0.6500\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 12s 542ms/step - loss: 0.6352 - accuracy: 0.6591 - val_loss: 0.6213 - val_accuracy: 0.7250\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 11s 502ms/step - loss: 0.5208 - accuracy: 0.7443 - val_loss: 0.6188 - val_accuracy: 0.7250\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 11s 522ms/step - loss: 0.4544 - accuracy: 0.8239 - val_loss: 0.6012 - val_accuracy: 0.6500\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 11s 492ms/step - loss: 0.4622 - accuracy: 0.8068 - val_loss: 0.5708 - val_accuracy: 0.7000\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 12s 537ms/step - loss: 0.4094 - accuracy: 0.8409 - val_loss: 0.5978 - val_accuracy: 0.8000\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 12s 526ms/step - loss: 0.3817 - accuracy: 0.8295 - val_loss: 0.5577 - val_accuracy: 0.8250\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 11s 495ms/step - loss: 0.3654 - accuracy: 0.8523 - val_loss: 0.5970 - val_accuracy: 0.6250\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 11s 503ms/step - loss: 0.3154 - accuracy: 0.8977 - val_loss: 0.5937 - val_accuracy: 0.6500\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 11s 510ms/step - loss: 0.3088 - accuracy: 0.8750 - val_loss: 0.6013 - val_accuracy: 0.6500\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.3294 - accuracy: 0.9034\n",
      "6/6 [==============================] - 2s 373ms/step - loss: 0.5724 - accuracy: 0.8182\n",
      "10/10 [==============================] - 5s 453ms/step - loss: 0.5311 - accuracy: 0.7625\n",
      "\n",
      "===== Training Base InceptionV3 - Epochs: 40, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base InceptionV3 with epochs=40, batch_size=16\n",
      "Epoch 1/40\n",
      "11/11 [==============================] - 18s 1s/step - loss: 0.7465 - accuracy: 0.5227 - val_loss: 0.6933 - val_accuracy: 0.5625\n",
      "Epoch 2/40\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.6386 - accuracy: 0.6761 - val_loss: 0.6756 - val_accuracy: 0.5938\n",
      "Epoch 3/40\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.5734 - accuracy: 0.7273 - val_loss: 0.6303 - val_accuracy: 0.6250\n",
      "Epoch 4/40\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.5446 - accuracy: 0.7330 - val_loss: 0.5295 - val_accuracy: 0.7812\n",
      "Epoch 5/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4666 - accuracy: 0.8011 - val_loss: 0.5286 - val_accuracy: 0.7500\n",
      "Epoch 6/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4687 - accuracy: 0.7955 - val_loss: 0.5408 - val_accuracy: 0.7188\n",
      "Epoch 7/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4305 - accuracy: 0.8352 - val_loss: 0.5241 - val_accuracy: 0.7188\n",
      "Epoch 8/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4067 - accuracy: 0.8295 - val_loss: 0.5725 - val_accuracy: 0.6875\n",
      "Epoch 9/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3944 - accuracy: 0.8466 - val_loss: 0.5722 - val_accuracy: 0.6250\n",
      "Epoch 10/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3635 - accuracy: 0.8750 - val_loss: 0.5637 - val_accuracy: 0.6875\n",
      "11/11 [==============================] - 9s 772ms/step - loss: 0.3672 - accuracy: 0.9148\n",
      "3/3 [==============================] - 2s 614ms/step - loss: 0.5565 - accuracy: 0.7045\n",
      "5/5 [==============================] - 5s 943ms/step - loss: 0.5290 - accuracy: 0.7625\n",
      "\n",
      "===== Training Base InceptionV3 - Epochs: 60, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base InceptionV3 with epochs=60, batch_size=8\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 18s 631ms/step - loss: 0.6772 - accuracy: 0.6136 - val_loss: 0.6041 - val_accuracy: 0.7500\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 11s 504ms/step - loss: 0.5569 - accuracy: 0.6989 - val_loss: 0.5899 - val_accuracy: 0.7250\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 12s 540ms/step - loss: 0.4574 - accuracy: 0.7955 - val_loss: 0.5861 - val_accuracy: 0.7750\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 12s 552ms/step - loss: 0.4483 - accuracy: 0.7955 - val_loss: 0.5082 - val_accuracy: 0.8000\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 11s 512ms/step - loss: 0.3960 - accuracy: 0.8239 - val_loss: 0.5608 - val_accuracy: 0.7500\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 11s 510ms/step - loss: 0.3234 - accuracy: 0.8920 - val_loss: 0.5410 - val_accuracy: 0.7750\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 11s 514ms/step - loss: 0.3064 - accuracy: 0.8807 - val_loss: 0.5503 - val_accuracy: 0.7250\n",
      "22/22 [==============================] - 9s 391ms/step - loss: 0.3471 - accuracy: 0.9091\n",
      "6/6 [==============================] - 2s 319ms/step - loss: 0.5424 - accuracy: 0.7727\n",
      "10/10 [==============================] - 5s 476ms/step - loss: 0.4985 - accuracy: 0.8125\n",
      "\n",
      "===== Training Base InceptionV3 - Epochs: 60, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base InceptionV3 with epochs=60, batch_size=16\n",
      "Epoch 1/60\n",
      "11/11 [==============================] - 18s 1s/step - loss: 0.7299 - accuracy: 0.5398 - val_loss: 0.6616 - val_accuracy: 0.6562\n",
      "Epoch 2/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.6012 - accuracy: 0.6705 - val_loss: 0.6691 - val_accuracy: 0.6562\n",
      "Epoch 3/60\n",
      "11/11 [==============================] - 11s 994ms/step - loss: 0.5351 - accuracy: 0.7386 - val_loss: 0.6037 - val_accuracy: 0.6562\n",
      "Epoch 4/60\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.4787 - accuracy: 0.8182 - val_loss: 0.5972 - val_accuracy: 0.7812\n",
      "Epoch 5/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4329 - accuracy: 0.8011 - val_loss: 0.5516 - val_accuracy: 0.6875\n",
      "Epoch 6/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4528 - accuracy: 0.8295 - val_loss: 0.6315 - val_accuracy: 0.7188\n",
      "Epoch 7/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3815 - accuracy: 0.8864 - val_loss: 0.6220 - val_accuracy: 0.6250\n",
      "Epoch 8/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3823 - accuracy: 0.8295 - val_loss: 0.5380 - val_accuracy: 0.7812\n",
      "Epoch 9/60\n",
      "11/11 [==============================] - 11s 983ms/step - loss: 0.3639 - accuracy: 0.8182 - val_loss: 0.5826 - val_accuracy: 0.6875\n",
      "Epoch 10/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3218 - accuracy: 0.8693 - val_loss: 0.5677 - val_accuracy: 0.6875\n",
      "Epoch 11/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3188 - accuracy: 0.8864 - val_loss: 0.4981 - val_accuracy: 0.7812\n",
      "Epoch 12/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2974 - accuracy: 0.8807 - val_loss: 0.5950 - val_accuracy: 0.7500\n",
      "Epoch 13/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2748 - accuracy: 0.9318 - val_loss: 0.5774 - val_accuracy: 0.6875\n",
      "Epoch 14/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2802 - accuracy: 0.8977 - val_loss: 0.5782 - val_accuracy: 0.6875\n",
      "11/11 [==============================] - 9s 768ms/step - loss: 0.2645 - accuracy: 0.9318\n",
      "3/3 [==============================] - 2s 627ms/step - loss: 0.5625 - accuracy: 0.7045\n",
      "5/5 [==============================] - 5s 967ms/step - loss: 0.4934 - accuracy: 0.7750\n",
      "\n",
      "======== Stage 2: Fine-tuning Models ========\n",
      "\n",
      "===== Fine-tuning InceptionV3 - Epochs: 20, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 62 layers out of 311 total layers in the base model.\n",
      "Training fine-tuned InceptionV3 with epochs=20, batch_size=8\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 19s 603ms/step - loss: 0.7114 - accuracy: 0.5000 - val_loss: 0.6995 - val_accuracy: 0.5750\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 12s 538ms/step - loss: 0.6229 - accuracy: 0.6591 - val_loss: 0.6689 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 12s 531ms/step - loss: 0.5392 - accuracy: 0.7500 - val_loss: 0.6278 - val_accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 11s 523ms/step - loss: 0.4896 - accuracy: 0.8125 - val_loss: 0.6268 - val_accuracy: 0.7000\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 11s 515ms/step - loss: 0.4394 - accuracy: 0.8466 - val_loss: 0.5779 - val_accuracy: 0.7250\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 12s 523ms/step - loss: 0.4055 - accuracy: 0.8750 - val_loss: 0.5521 - val_accuracy: 0.7500\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 13s 584ms/step - loss: 0.3448 - accuracy: 0.9091 - val_loss: 0.5629 - val_accuracy: 0.7500\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 14s 647ms/step - loss: 0.3089 - accuracy: 0.9432 - val_loss: 0.5203 - val_accuracy: 0.7750\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 12s 528ms/step - loss: 0.2602 - accuracy: 0.9545 - val_loss: 0.5538 - val_accuracy: 0.7250\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 12s 561ms/step - loss: 0.2317 - accuracy: 0.9659 - val_loss: 0.5139 - val_accuracy: 0.8000\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 12s 515ms/step - loss: 0.2270 - accuracy: 0.9602 - val_loss: 0.5222 - val_accuracy: 0.7750\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 11s 513ms/step - loss: 0.2145 - accuracy: 0.9489 - val_loss: 0.4982 - val_accuracy: 0.7750\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 11s 502ms/step - loss: 0.1560 - accuracy: 0.9716 - val_loss: 0.4743 - val_accuracy: 0.7750\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 11s 509ms/step - loss: 0.1474 - accuracy: 0.9773 - val_loss: 0.4633 - val_accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 11s 499ms/step - loss: 0.1471 - accuracy: 0.9773 - val_loss: 0.4765 - val_accuracy: 0.7500\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 11s 498ms/step - loss: 0.0896 - accuracy: 0.9943 - val_loss: 0.4655 - val_accuracy: 0.8000\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 12s 532ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.4363 - val_accuracy: 0.8250\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 11s 507ms/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 0.4688 - val_accuracy: 0.8000\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 11s 506ms/step - loss: 0.1194 - accuracy: 0.9716 - val_loss: 0.4876 - val_accuracy: 0.7750\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 11s 505ms/step - loss: 0.0811 - accuracy: 0.9943 - val_loss: 0.5224 - val_accuracy: 0.7250\n",
      "22/22 [==============================] - 9s 391ms/step - loss: 0.0379 - accuracy: 1.0000\n",
      "6/6 [==============================] - 2s 361ms/step - loss: 0.4756 - accuracy: 0.7955\n",
      "10/10 [==============================] - 5s 445ms/step - loss: 0.4591 - accuracy: 0.7750\n",
      "\n",
      "===== Fine-tuning InceptionV3 - Epochs: 20, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 62 layers out of 311 total layers in the base model.\n",
      "Training fine-tuned InceptionV3 with epochs=20, batch_size=16\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - 18s 1s/step - loss: 0.7748 - accuracy: 0.4773 - val_loss: 0.7572 - val_accuracy: 0.5312\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.6446 - accuracy: 0.6136 - val_loss: 0.7200 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.6194 - accuracy: 0.6648 - val_loss: 0.6829 - val_accuracy: 0.5625\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.5388 - accuracy: 0.7898 - val_loss: 0.7039 - val_accuracy: 0.5312\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.4976 - accuracy: 0.8523 - val_loss: 0.6362 - val_accuracy: 0.6562\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4508 - accuracy: 0.9091 - val_loss: 0.6196 - val_accuracy: 0.6250\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4273 - accuracy: 0.8750 - val_loss: 0.6306 - val_accuracy: 0.6562\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.3674 - accuracy: 0.9489 - val_loss: 0.6035 - val_accuracy: 0.7188\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3072 - accuracy: 0.9830 - val_loss: 0.5839 - val_accuracy: 0.7188\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.3123 - accuracy: 0.9659 - val_loss: 0.5888 - val_accuracy: 0.7500\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2682 - accuracy: 0.9886 - val_loss: 0.4800 - val_accuracy: 0.8125\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.2381 - accuracy: 0.9830 - val_loss: 0.5485 - val_accuracy: 0.7812\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.2140 - accuracy: 0.9886 - val_loss: 0.4858 - val_accuracy: 0.8438\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 11s 1000ms/step - loss: 0.1893 - accuracy: 0.9943 - val_loss: 0.5306 - val_accuracy: 0.8125\n",
      "11/11 [==============================] - 9s 784ms/step - loss: 0.2452 - accuracy: 0.9830\n",
      "3/3 [==============================] - 2s 716ms/step - loss: 0.5776 - accuracy: 0.7500\n",
      "5/5 [==============================] - 5s 946ms/step - loss: 0.5740 - accuracy: 0.6375\n",
      "\n",
      "===== Fine-tuning InceptionV3 - Epochs: 40, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 62 layers out of 311 total layers in the base model.\n",
      "Training fine-tuned InceptionV3 with epochs=40, batch_size=8\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 18s 611ms/step - loss: 0.7358 - accuracy: 0.4659 - val_loss: 0.7277 - val_accuracy: 0.3750\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 12s 529ms/step - loss: 0.6120 - accuracy: 0.6705 - val_loss: 0.7328 - val_accuracy: 0.4750\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 12s 555ms/step - loss: 0.5648 - accuracy: 0.7557 - val_loss: 0.6806 - val_accuracy: 0.5500\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 11s 518ms/step - loss: 0.4870 - accuracy: 0.8409 - val_loss: 0.6740 - val_accuracy: 0.5500\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 12s 550ms/step - loss: 0.4185 - accuracy: 0.8977 - val_loss: 0.6656 - val_accuracy: 0.5750\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 11s 505ms/step - loss: 0.3863 - accuracy: 0.8977 - val_loss: 0.6340 - val_accuracy: 0.5750\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 12s 575ms/step - loss: 0.3251 - accuracy: 0.9489 - val_loss: 0.5762 - val_accuracy: 0.7000\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 13s 581ms/step - loss: 0.3129 - accuracy: 0.9261 - val_loss: 0.5895 - val_accuracy: 0.7500\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 12s 551ms/step - loss: 0.2645 - accuracy: 0.9261 - val_loss: 0.5928 - val_accuracy: 0.7250\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 12s 561ms/step - loss: 0.2344 - accuracy: 0.9659 - val_loss: 0.5943 - val_accuracy: 0.7500\n",
      "22/22 [==============================] - 10s 426ms/step - loss: 0.2416 - accuracy: 0.9886\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 0.5902 - accuracy: 0.6818\n",
      "10/10 [==============================] - 5s 503ms/step - loss: 0.5560 - accuracy: 0.7125\n",
      "\n",
      "===== Fine-tuning InceptionV3 - Epochs: 40, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 62 layers out of 311 total layers in the base model.\n",
      "Training fine-tuned InceptionV3 with epochs=40, batch_size=16\n",
      "Epoch 1/40\n",
      "11/11 [==============================] - 20s 1s/step - loss: 0.7655 - accuracy: 0.5170 - val_loss: 0.7351 - val_accuracy: 0.4375\n",
      "Epoch 2/40\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.6698 - accuracy: 0.6080 - val_loss: 0.6720 - val_accuracy: 0.5625\n",
      "Epoch 3/40\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.5917 - accuracy: 0.7045 - val_loss: 0.6292 - val_accuracy: 0.6562\n",
      "Epoch 4/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5261 - accuracy: 0.8068 - val_loss: 0.6465 - val_accuracy: 0.5312\n",
      "Epoch 5/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4632 - accuracy: 0.8466 - val_loss: 0.6495 - val_accuracy: 0.5625\n",
      "Epoch 6/40\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.4536 - accuracy: 0.9034 - val_loss: 0.5758 - val_accuracy: 0.7188\n",
      "Epoch 7/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4032 - accuracy: 0.9034 - val_loss: 0.6496 - val_accuracy: 0.5938\n",
      "Epoch 8/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3423 - accuracy: 0.9489 - val_loss: 0.6246 - val_accuracy: 0.6250\n",
      "Epoch 9/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3199 - accuracy: 0.9602 - val_loss: 0.5689 - val_accuracy: 0.6875\n",
      "Epoch 10/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2921 - accuracy: 0.9659 - val_loss: 0.5933 - val_accuracy: 0.6562\n",
      "Epoch 11/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2363 - accuracy: 0.9830 - val_loss: 0.5892 - val_accuracy: 0.7188\n",
      "Epoch 12/40\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.1940 - accuracy: 1.0000 - val_loss: 0.5809 - val_accuracy: 0.6250\n",
      "11/11 [==============================] - 9s 791ms/step - loss: 0.3072 - accuracy: 0.9375\n",
      "3/3 [==============================] - 2s 775ms/step - loss: 0.5967 - accuracy: 0.6818\n",
      "5/5 [==============================] - 5s 970ms/step - loss: 0.6147 - accuracy: 0.6250\n",
      "\n",
      "===== Fine-tuning InceptionV3 - Epochs: 60, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 62 layers out of 311 total layers in the base model.\n",
      "Training fine-tuned InceptionV3 with epochs=60, batch_size=8\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 19s 639ms/step - loss: 0.7432 - accuracy: 0.4659 - val_loss: 0.6738 - val_accuracy: 0.5750\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 12s 539ms/step - loss: 0.6238 - accuracy: 0.6080 - val_loss: 0.6576 - val_accuracy: 0.6000\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 12s 554ms/step - loss: 0.5602 - accuracy: 0.7727 - val_loss: 0.6282 - val_accuracy: 0.6500\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 11s 520ms/step - loss: 0.5144 - accuracy: 0.8182 - val_loss: 0.6033 - val_accuracy: 0.6500\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 12s 527ms/step - loss: 0.4908 - accuracy: 0.8125 - val_loss: 0.5771 - val_accuracy: 0.6250\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 11s 514ms/step - loss: 0.4506 - accuracy: 0.8750 - val_loss: 0.5870 - val_accuracy: 0.6500\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 11s 507ms/step - loss: 0.3802 - accuracy: 0.8977 - val_loss: 0.5789 - val_accuracy: 0.6500\n",
      "Epoch 8/60\n",
      "22/22 [==============================] - 12s 556ms/step - loss: 0.3207 - accuracy: 0.9602 - val_loss: 0.5351 - val_accuracy: 0.7000\n",
      "Epoch 9/60\n",
      "22/22 [==============================] - 11s 525ms/step - loss: 0.2482 - accuracy: 0.9659 - val_loss: 0.5301 - val_accuracy: 0.6750\n",
      "Epoch 10/60\n",
      "22/22 [==============================] - 11s 511ms/step - loss: 0.2866 - accuracy: 0.9318 - val_loss: 0.5096 - val_accuracy: 0.7000\n",
      "Epoch 11/60\n",
      "22/22 [==============================] - 11s 512ms/step - loss: 0.2117 - accuracy: 0.9602 - val_loss: 0.5396 - val_accuracy: 0.6500\n",
      "Epoch 12/60\n",
      "22/22 [==============================] - 11s 517ms/step - loss: 0.2063 - accuracy: 0.9432 - val_loss: 0.4934 - val_accuracy: 0.7000\n",
      "Epoch 13/60\n",
      "22/22 [==============================] - 11s 511ms/step - loss: 0.1450 - accuracy: 0.9943 - val_loss: 0.5078 - val_accuracy: 0.7000\n",
      "Epoch 14/60\n",
      "22/22 [==============================] - 11s 512ms/step - loss: 0.1397 - accuracy: 0.9886 - val_loss: 0.5100 - val_accuracy: 0.7000\n",
      "Epoch 15/60\n",
      "22/22 [==============================] - 12s 532ms/step - loss: 0.0934 - accuracy: 0.9943 - val_loss: 0.5152 - val_accuracy: 0.7000\n",
      "22/22 [==============================] - 9s 405ms/step - loss: 0.1038 - accuracy: 1.0000\n",
      "6/6 [==============================] - 2s 352ms/step - loss: 0.4966 - accuracy: 0.6818\n",
      "10/10 [==============================] - 5s 449ms/step - loss: 0.5388 - accuracy: 0.7250\n",
      "\n",
      "===== Fine-tuning InceptionV3 - Epochs: 60, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 62 layers out of 311 total layers in the base model.\n",
      "Training fine-tuned InceptionV3 with epochs=60, batch_size=16\n",
      "Epoch 1/60\n",
      "11/11 [==============================] - 19s 1s/step - loss: 0.7357 - accuracy: 0.5284 - val_loss: 0.6864 - val_accuracy: 0.5625\n",
      "Epoch 2/60\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.6570 - accuracy: 0.6023 - val_loss: 0.6757 - val_accuracy: 0.5000\n",
      "Epoch 3/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5786 - accuracy: 0.7386 - val_loss: 0.6146 - val_accuracy: 0.5625\n",
      "Epoch 4/60\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.5488 - accuracy: 0.7557 - val_loss: 0.6119 - val_accuracy: 0.6875\n",
      "Epoch 5/60\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.4800 - accuracy: 0.8409 - val_loss: 0.5553 - val_accuracy: 0.7188\n",
      "Epoch 6/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4281 - accuracy: 0.8693 - val_loss: 0.6096 - val_accuracy: 0.6875\n",
      "Epoch 7/60\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.3813 - accuracy: 0.9091 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 8/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3236 - accuracy: 0.9545 - val_loss: 0.5315 - val_accuracy: 0.7188\n",
      "Epoch 9/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3065 - accuracy: 0.9602 - val_loss: 0.5479 - val_accuracy: 0.6562\n",
      "Epoch 10/60\n",
      "11/11 [==============================] - 11s 982ms/step - loss: 0.2574 - accuracy: 0.9773 - val_loss: 0.5971 - val_accuracy: 0.6562\n",
      "11/11 [==============================] - 9s 791ms/step - loss: 0.3840 - accuracy: 0.8466\n",
      "3/3 [==============================] - 2s 750ms/step - loss: 0.5758 - accuracy: 0.6818\n",
      "5/5 [==============================] - 5s 928ms/step - loss: 0.6560 - accuracy: 0.6125\n",
      "\n",
      "All processes completed successfully!\n",
      "Results saved to: C:/Users/user/Desktop/MLWORK/InceptionV3\n"
     ]
    }
   ],
   "source": [
    "# 主函數\n",
    "def main():\n",
    "    # 創建CSV結果文件\n",
    "    csv_files = create_result_csv_files()\n",
    "    \n",
    "    model_name = \"InceptionV3\"\n",
    "    all_results = {}  # 儲存所有結果以供後續比較\n",
    "    \n",
    "    # 第1階段：訓練和評估基礎模型\n",
    "    print(\"======== Stage 1: Training and Evaluating Base Models ========\")\n",
    "    \n",
    "    for epochs in epochs_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            print(f\"\\n===== Training Base {model_name} - Epochs: {epochs}, Batch Size: {batch_size} =====\")\n",
    "            \n",
    "            # 創建資料生成器\n",
    "            train_gen, val_gen, test_gen = create_data_generators(batch_size)\n",
    "            \n",
    "            # 創建基礎模型\n",
    "            base_model = create_inception_model()\n",
    "            \n",
    "            # 訓練和評估基礎模型\n",
    "            base_results = train_and_evaluate_model(\n",
    "                base_model, train_gen, val_gen, test_gen,\n",
    "                model_name, epochs, batch_size, is_fine_tuned=False\n",
    "            )\n",
    "            \n",
    "            # 保存結果到CSV\n",
    "            save_results_to_csv(\n",
    "                base_results, csv_files['base'], \n",
    "                model_name, epochs, is_fine_tuned=False\n",
    "            )\n",
    "            \n",
    "            # 保存模型結果以供後續比較\n",
    "            result_key = f\"{model_name}_e{epochs}_b{batch_size}\"\n",
    "            all_results[result_key] = {'base': base_results}\n",
    "            \n",
    "            # 清理內存\n",
    "            tf.keras.backend.clear_session()\n",
    "    \n",
    "    # 第2階段：微調模型\n",
    "    print(\"\\n======== Stage 2: Fine-tuning Models ========\")\n",
    "    \n",
    "    for epochs in epochs_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            print(f\"\\n===== Fine-tuning {model_name} - Epochs: {epochs}, Batch Size: {batch_size} =====\")\n",
    "            \n",
    "            # 創建資料生成器 (重新創建是為了確保數據的一致性)\n",
    "            train_gen, val_gen, test_gen = create_data_generators(batch_size)\n",
    "            \n",
    "            # 創建模型\n",
    "            tuned_model = create_inception_model()\n",
    "            \n",
    "            # 微調模型\n",
    "            tuned_model = fine_tune_model(tuned_model)\n",
    "            \n",
    "            # 訓練和評估微調後的模型\n",
    "            tuned_results = train_and_evaluate_model(\n",
    "                tuned_model, train_gen, val_gen, test_gen,\n",
    "                model_name, epochs, batch_size, is_fine_tuned=True\n",
    "            )\n",
    "            \n",
    "            # 保存結果到CSV\n",
    "            save_results_to_csv(\n",
    "                tuned_results, csv_files['tuned'], \n",
    "                model_name, epochs, is_fine_tuned=True\n",
    "            )\n",
    "            \n",
    "            # 添加到結果字典\n",
    "            result_key = f\"{model_name}_e{epochs}_b{batch_size}\"\n",
    "            if result_key in all_results:\n",
    "                all_results[result_key]['tuned'] = tuned_results\n",
    "                \n",
    "                # 保存比較結果\n",
    "                base_results = all_results[result_key]['base']\n",
    "                save_comparison_to_csv(\n",
    "                    base_results, tuned_results, csv_files['comparison'],\n",
    "                    model_name, epochs\n",
    "                )\n",
    "            \n",
    "            # 清理內存\n",
    "            tf.keras.backend.clear_session()\n",
    "    \n",
    "    print(\"\\nAll processes completed successfully!\")\n",
    "    print(f\"Results saved to: {results_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa559682-58cf-4faf-b121-b2435c9856ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
