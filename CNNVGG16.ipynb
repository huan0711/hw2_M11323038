{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dde45e0-ce3d-4245-9f6b-d8fcbe49198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import tensorflow  as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,UpSampling2D,InputLayer,Reshape\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from keras.layers import Dropout,Activation,BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.utils import array_to_img\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, GlobalAveragePooling2D\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.utils import Bunch\n",
    "import math\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee1b247-5f16-475f-843b-d9a31ccadc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定圖片大小和路徑\n",
    "img_width, img_height = 224, 224 # VGG和ResNet的標準輸入大小\n",
    "train_data_dir = \"C:/Users/user/Desktop/MLwork2/train\"\n",
    "test_data_dir = \"C:/Users/user/Desktop/MLwork2/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70eafecf-4b80-41a7-866c-3da0945d6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [20, 40, 60]\n",
    "batch_size_list = [8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32abe247-a115-4002-b821-bbab84ae4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'C:/Users/user/Desktop/MLWORK/VGG16'\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d76297d-d8ab-4f40-bad0-eeb053692b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建CSV檔案記錄結果\n",
    "def create_result_csv_files():\n",
    "    csv_files = {}\n",
    "    \n",
    "    # 創建基本模型結果CSV\n",
    "    base_csv_path = os.path.join(results_dir, \"VGG16_base_model_results.csv\")\n",
    "    with open(base_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Model', 'Epochs', 'Train Accuracy', 'Train Loss', 'Val Accuracy', 'Val Loss', 'Test Accuracy', 'Test Loss'])\n",
    "    csv_files['base'] = base_csv_path\n",
    "    \n",
    "    # 創建微調模型結果CSV\n",
    "    tuned_csv_path = os.path.join(results_dir, \"VGG16_fine_tuned_model_results.csv\")\n",
    "    with open(tuned_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Model', 'Epochs', 'Train Accuracy', 'Train Loss', 'Val Accuracy', 'Val Loss', 'Test Accuracy', 'Test Loss'])\n",
    "    csv_files['tuned'] = tuned_csv_path\n",
    "    \n",
    "    # 創建模型比較CSV\n",
    "    comparison_csv_path = os.path.join(results_dir, \"VGG16_tuned_or_not_comparison.csv\")\n",
    "    with open(comparison_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Model', 'Epochs', 'Base Accuracy', 'Base Loss', 'Tuned Accuracy', 'Tuned Loss', 'Improvement'])\n",
    "    csv_files['comparison'] = comparison_csv_path\n",
    "    \n",
    "    return csv_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fa8356-58b2-417c-81bd-f52dbc5f3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料生成器 (無資料增強)\n",
    "def create_data_generators(batch_size):\n",
    "    # 僅做標準化處理，不增強數據\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255,\n",
    "        validation_split=0.2  # 保留驗證分割\n",
    "    )\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    val_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "198cf5c9-3ee0-4818-b22e-a849b2156475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建VGG16模型\n",
    "def create_vgg_model():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "    \n",
    "    # 凍結基礎模型的層\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # 添加新的分類層\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  # 二元分類\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b67457e-0938-4d1d-9d8d-d536d5b6b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微調模型：解凍部分基礎模型的層\n",
    "def fine_tune_model(model):\n",
    "    # 計算要解凍的層數（所有模型使用相同的比例：頂部20%的層）\n",
    "    # 確保 model 是一個 Sequential 模型且第一個層是 base_model\n",
    "    if isinstance(model, Sequential) and len(model.layers) > 0:\n",
    "        base_model = model.layers[0] # <-- 正確地獲取 VGG16 基礎模型\n",
    "    else:\n",
    "        print(\"Warning: model structure unexpected, could not identify base_model for fine-tuning.\")\n",
    "        return model\n",
    "\n",
    "    if not hasattr(base_model, 'layers'):\n",
    "        print(\"Warning: base_model does not have layers to unfreeze.\")\n",
    "        return model\n",
    "\n",
    "    total_layers = len(base_model.layers) # <-- 計算 VGG16 基礎模型的層數\n",
    "    if total_layers == 0:\n",
    "         print(\"Warning: base_model has no layers to unfreeze.\")\n",
    "         return model\n",
    "\n",
    "    unfreeze_layers = max(0, int(total_layers * 0.2))  # 解凍頂部20%的層\n",
    "\n",
    "    # 先凍結所有基礎模型的層\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 然後解凍 VGG16 基礎模型頂部的指定層數\n",
    "    for layer in base_model.layers[-unfreeze_layers:]: # <-- 解凍 VGG16 基礎模型的頂部層\n",
    "        layer.trainable = True\n",
    "\n",
    "    print(f\"Fine-tuning model: Unfrozen {unfreeze_layers} layers out of {total_layers} total layers in the base model.\")\n",
    "\n",
    "    # 模型編譯在 train_and_evaluate_model 函式中處理\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5827a7bc-c1b0-4d95-b952-ae498eb2d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練和評估模型\n",
    "def train_and_evaluate_model(model, train_generator, val_generator, test_generator, \n",
    "                           model_name, epochs, batch_size, is_fine_tuned=False):\n",
    "    model_prefix = \"fine_tuned_\" if is_fine_tuned else \"base_\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "        ModelCheckpoint(\n",
    "            os.path.join(results_dir, f\"{model_prefix}{model_name}_e{epochs}.h5\"),\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 訓練模型\n",
    "    print(f\"Training {'fine-tuned' if is_fine_tuned else 'base'} {model_name} with epochs={epochs}, batch_size={batch_size}\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_generator.samples // batch_size,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # 評估訓練集\n",
    "    train_eval = model.evaluate(train_generator)\n",
    "    train_loss, train_acc = train_eval\n",
    "    \n",
    "    # 評估驗證集\n",
    "    val_eval = model.evaluate(val_generator)\n",
    "    val_loss, val_acc = val_eval\n",
    "    \n",
    "    # 評估測試集\n",
    "    test_eval = model.evaluate(test_generator)\n",
    "    test_loss, test_acc = test_eval\n",
    "    \n",
    "    # 繪製訓練歷史\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 準確率圖\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], 'b', label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], 'r', label='Validation Accuracy')\n",
    "    plt.title(f'{model_prefix.capitalize()} {model_name} - Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 損失函數圖\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], 'b', label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], 'r', label='Validation Loss')\n",
    "    plt.title(f'{model_prefix.capitalize()} {model_name} - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f\"{model_prefix.lower()}_{model_name.lower()}_e{epochs}_b{batch_size}_history.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'train_accuracy': train_acc,\n",
    "        'train_loss': train_loss,\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d10ab06-06f1-4a4c-a6c7-218a69db1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將結果保存到CSV\n",
    "def save_results_to_csv(results, csv_path, model_name, epochs, is_fine_tuned=False):\n",
    "    with open(csv_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            model_name,\n",
    "            epochs,\n",
    "            results['train_accuracy'],\n",
    "            results['train_loss'],\n",
    "            results['val_accuracy'],\n",
    "            results['val_loss'],\n",
    "            results['test_accuracy'],\n",
    "            results['test_loss']\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8543e673-e1de-4677-9a68-e309e12f46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存比較結果到CSV\n",
    "def save_comparison_to_csv(base_results, tuned_results, csv_path, model_name, epochs):\n",
    "    improvement = tuned_results['test_accuracy'] - base_results['test_accuracy']\n",
    "    \n",
    "    with open(csv_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            model_name,\n",
    "            epochs,\n",
    "            base_results['test_accuracy'],\n",
    "            base_results['test_loss'],\n",
    "            tuned_results['test_accuracy'],\n",
    "            tuned_results['test_loss'],\n",
    "            improvement\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f02ad10c-7f23-4b5e-9235-c180e7c54c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Stage 1: Training and Evaluating Base Models ========\n",
      "\n",
      "===== Training Base VGG16 - Epochs: 20, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base VGG16 with epochs=20, batch_size=8\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 20s 550ms/step - loss: 0.8161 - accuracy: 0.5511 - val_loss: 0.5559 - val_accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 11s 522ms/step - loss: 0.4783 - accuracy: 0.7784 - val_loss: 0.5176 - val_accuracy: 0.7750\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 11s 496ms/step - loss: 0.3736 - accuracy: 0.8295 - val_loss: 0.5016 - val_accuracy: 0.7750\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 11s 512ms/step - loss: 0.3185 - accuracy: 0.8693 - val_loss: 0.4886 - val_accuracy: 0.7000\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 11s 502ms/step - loss: 0.2344 - accuracy: 0.9034 - val_loss: 0.4652 - val_accuracy: 0.7500\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 11s 526ms/step - loss: 0.1782 - accuracy: 0.9545 - val_loss: 0.4542 - val_accuracy: 0.8000\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 11s 499ms/step - loss: 0.1196 - accuracy: 0.9830 - val_loss: 0.5188 - val_accuracy: 0.7250\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 11s 490ms/step - loss: 0.0953 - accuracy: 0.9830 - val_loss: 0.4728 - val_accuracy: 0.7250\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 11s 496ms/step - loss: 0.0715 - accuracy: 0.9886 - val_loss: 0.4780 - val_accuracy: 0.7500\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.1015 - accuracy: 0.9886\n",
      "6/6 [==============================] - 3s 513ms/step - loss: 0.4784 - accuracy: 0.7727\n",
      "10/10 [==============================] - 5s 473ms/step - loss: 0.6059 - accuracy: 0.7375\n",
      "\n",
      "===== Training Base VGG16 - Epochs: 20, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base VGG16 with epochs=20, batch_size=16\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.8868 - accuracy: 0.5568 - val_loss: 0.6459 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5971 - accuracy: 0.6591 - val_loss: 0.6232 - val_accuracy: 0.5938\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.4383 - accuracy: 0.8125 - val_loss: 0.5544 - val_accuracy: 0.7188\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3556 - accuracy: 0.8750 - val_loss: 0.5306 - val_accuracy: 0.7188\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2531 - accuracy: 0.9489 - val_loss: 0.4814 - val_accuracy: 0.8125\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2311 - accuracy: 0.9432 - val_loss: 0.5066 - val_accuracy: 0.7188\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 11s 995ms/step - loss: 0.2139 - accuracy: 0.9375 - val_loss: 0.4906 - val_accuracy: 0.7188\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1586 - accuracy: 0.9659 - val_loss: 0.4618 - val_accuracy: 0.7812\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1503 - accuracy: 0.9716 - val_loss: 0.4720 - val_accuracy: 0.6562\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1157 - accuracy: 0.9886 - val_loss: 0.4775 - val_accuracy: 0.7188\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1171 - accuracy: 0.9886 - val_loss: 0.4585 - val_accuracy: 0.7812\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.0901 - accuracy: 1.0000 - val_loss: 0.5289 - val_accuracy: 0.6875\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.0884 - accuracy: 0.9943 - val_loss: 0.4937 - val_accuracy: 0.7188\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.0753 - accuracy: 0.9943 - val_loss: 0.3651 - val_accuracy: 0.8438\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 11s 999ms/step - loss: 0.0683 - accuracy: 0.9830 - val_loss: 0.3940 - val_accuracy: 0.8125\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 11s 998ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.4270 - val_accuracy: 0.7500\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.5054 - val_accuracy: 0.7500\n",
      "11/11 [==============================] - 9s 759ms/step - loss: 0.0515 - accuracy: 1.0000\n",
      "3/3 [==============================] - 4s 2s/step - loss: 0.4563 - accuracy: 0.7727\n",
      "5/5 [==============================] - 5s 983ms/step - loss: 0.4751 - accuracy: 0.7750\n",
      "\n",
      "===== Training Base VGG16 - Epochs: 40, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base VGG16 with epochs=40, batch_size=8\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 12s 530ms/step - loss: 0.9219 - accuracy: 0.4886 - val_loss: 0.7277 - val_accuracy: 0.5750\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 11s 516ms/step - loss: 0.5054 - accuracy: 0.7614 - val_loss: 0.5961 - val_accuracy: 0.6750\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 11s 525ms/step - loss: 0.3591 - accuracy: 0.8523 - val_loss: 0.5681 - val_accuracy: 0.7500\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 11s 494ms/step - loss: 0.2415 - accuracy: 0.9318 - val_loss: 0.5387 - val_accuracy: 0.7000\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 11s 503ms/step - loss: 0.1995 - accuracy: 0.9545 - val_loss: 0.5174 - val_accuracy: 0.7250\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 11s 506ms/step - loss: 0.1664 - accuracy: 0.9602 - val_loss: 0.4949 - val_accuracy: 0.7500\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 11s 501ms/step - loss: 0.1356 - accuracy: 0.9659 - val_loss: 0.5779 - val_accuracy: 0.6750\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 11s 506ms/step - loss: 0.0942 - accuracy: 0.9886 - val_loss: 0.4767 - val_accuracy: 0.8000\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 11s 498ms/step - loss: 0.0778 - accuracy: 0.9886 - val_loss: 0.4440 - val_accuracy: 0.7750\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 11s 499ms/step - loss: 0.0585 - accuracy: 1.0000 - val_loss: 0.4513 - val_accuracy: 0.8000\n",
      "Epoch 11/40\n",
      "22/22 [==============================] - 11s 501ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.4771 - val_accuracy: 0.8000\n",
      "Epoch 12/40\n",
      "22/22 [==============================] - 11s 499ms/step - loss: 0.0530 - accuracy: 0.9943 - val_loss: 0.4828 - val_accuracy: 0.7750\n",
      "22/22 [==============================] - 9s 385ms/step - loss: 0.0437 - accuracy: 1.0000\n",
      "6/6 [==============================] - 2s 349ms/step - loss: 0.4723 - accuracy: 0.7727\n",
      "10/10 [==============================] - 5s 458ms/step - loss: 0.5340 - accuracy: 0.8125\n",
      "\n",
      "===== Training Base VGG16 - Epochs: 40, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base VGG16 with epochs=40, batch_size=16\n",
      "Epoch 1/40\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.7732 - accuracy: 0.4943 - val_loss: 0.6007 - val_accuracy: 0.7188\n",
      "Epoch 2/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5160 - accuracy: 0.7443 - val_loss: 0.5675 - val_accuracy: 0.6562\n",
      "Epoch 3/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3477 - accuracy: 0.8636 - val_loss: 0.5469 - val_accuracy: 0.6875\n",
      "Epoch 4/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2690 - accuracy: 0.9148 - val_loss: 0.5060 - val_accuracy: 0.6875\n",
      "Epoch 5/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2427 - accuracy: 0.9318 - val_loss: 0.5490 - val_accuracy: 0.6562\n",
      "Epoch 6/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2128 - accuracy: 0.9375 - val_loss: 0.5095 - val_accuracy: 0.7500\n",
      "Epoch 7/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1637 - accuracy: 0.9375 - val_loss: 0.4100 - val_accuracy: 0.7812\n",
      "Epoch 8/40\n",
      "11/11 [==============================] - 11s 995ms/step - loss: 0.1299 - accuracy: 0.9716 - val_loss: 0.4740 - val_accuracy: 0.7500\n",
      "Epoch 9/40\n",
      "11/11 [==============================] - 11s 996ms/step - loss: 0.0968 - accuracy: 0.9830 - val_loss: 0.4323 - val_accuracy: 0.7812\n",
      "Epoch 10/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.0885 - accuracy: 0.9830 - val_loss: 0.5320 - val_accuracy: 0.8125\n",
      "11/11 [==============================] - 9s 787ms/step - loss: 0.0908 - accuracy: 0.9943\n",
      "3/3 [==============================] - 2s 617ms/step - loss: 0.4813 - accuracy: 0.7500\n",
      "5/5 [==============================] - 5s 914ms/step - loss: 0.5057 - accuracy: 0.7625\n",
      "\n",
      "===== Training Base VGG16 - Epochs: 60, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base VGG16 with epochs=60, batch_size=8\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 13s 551ms/step - loss: 0.9174 - accuracy: 0.5170 - val_loss: 0.8060 - val_accuracy: 0.5250\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 11s 524ms/step - loss: 0.4950 - accuracy: 0.7727 - val_loss: 0.5240 - val_accuracy: 0.7250\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 11s 499ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.6533 - val_accuracy: 0.6000\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 11s 498ms/step - loss: 0.3026 - accuracy: 0.8864 - val_loss: 0.5584 - val_accuracy: 0.7250\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 11s 521ms/step - loss: 0.2244 - accuracy: 0.9375 - val_loss: 0.4571 - val_accuracy: 0.7500\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 11s 515ms/step - loss: 0.1749 - accuracy: 0.9432 - val_loss: 0.4563 - val_accuracy: 0.8000\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 11s 490ms/step - loss: 0.1110 - accuracy: 0.9830 - val_loss: 0.5209 - val_accuracy: 0.7750\n",
      "Epoch 8/60\n",
      "22/22 [==============================] - 11s 487ms/step - loss: 0.1019 - accuracy: 0.9773 - val_loss: 0.5042 - val_accuracy: 0.7750\n",
      "Epoch 9/60\n",
      "22/22 [==============================] - 11s 506ms/step - loss: 0.1033 - accuracy: 0.9773 - val_loss: 0.4889 - val_accuracy: 0.8000\n",
      "22/22 [==============================] - 9s 388ms/step - loss: 0.0903 - accuracy: 1.0000\n",
      "6/6 [==============================] - 2s 396ms/step - loss: 0.4894 - accuracy: 0.7727\n",
      "10/10 [==============================] - 5s 459ms/step - loss: 0.5227 - accuracy: 0.7625\n",
      "\n",
      "===== Training Base VGG16 - Epochs: 60, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Training base VGG16 with epochs=60, batch_size=16\n",
      "Epoch 1/60\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.8083 - accuracy: 0.5227 - val_loss: 0.6479 - val_accuracy: 0.6562\n",
      "Epoch 2/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5089 - accuracy: 0.7557 - val_loss: 0.6202 - val_accuracy: 0.6250\n",
      "Epoch 3/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3562 - accuracy: 0.8750 - val_loss: 0.5433 - val_accuracy: 0.6875\n",
      "Epoch 4/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3180 - accuracy: 0.8693 - val_loss: 0.5328 - val_accuracy: 0.6562\n",
      "Epoch 5/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2446 - accuracy: 0.9261 - val_loss: 0.5227 - val_accuracy: 0.6250\n",
      "Epoch 6/60\n",
      "11/11 [==============================] - 11s 943ms/step - loss: 0.1765 - accuracy: 0.9716 - val_loss: 0.5468 - val_accuracy: 0.6562\n",
      "Epoch 7/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1463 - accuracy: 0.9716 - val_loss: 0.5392 - val_accuracy: 0.7188\n",
      "Epoch 8/60\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.1223 - accuracy: 0.9773 - val_loss: 0.4800 - val_accuracy: 0.7500\n",
      "Epoch 9/60\n",
      "11/11 [==============================] - 11s 985ms/step - loss: 0.1031 - accuracy: 0.9773 - val_loss: 0.5546 - val_accuracy: 0.6875\n",
      "Epoch 10/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.0799 - accuracy: 0.9830 - val_loss: 0.4766 - val_accuracy: 0.7500\n",
      "Epoch 11/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.0714 - accuracy: 0.9886 - val_loss: 0.4725 - val_accuracy: 0.8125\n",
      "Epoch 12/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.0677 - accuracy: 0.9886 - val_loss: 0.3450 - val_accuracy: 0.8750\n",
      "Epoch 13/60\n",
      "11/11 [==============================] - 11s 961ms/step - loss: 0.0524 - accuracy: 0.9943 - val_loss: 0.4816 - val_accuracy: 0.7812\n",
      "Epoch 14/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.4769 - val_accuracy: 0.7500\n",
      "Epoch 15/60\n",
      "11/11 [==============================] - 11s 978ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.5160 - val_accuracy: 0.7188\n",
      "11/11 [==============================] - 9s 733ms/step - loss: 0.0392 - accuracy: 1.0000\n",
      "3/3 [==============================] - 2s 562ms/step - loss: 0.4907 - accuracy: 0.7727\n",
      "5/5 [==============================] - 5s 934ms/step - loss: 0.5455 - accuracy: 0.7500\n",
      "\n",
      "======== Stage 2: Fine-tuning Models ========\n",
      "\n",
      "===== Fine-tuning VGG16 - Epochs: 20, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 3 layers out of 19 total layers in the base model.\n",
      "Training fine-tuned VGG16 with epochs=20, batch_size=8\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 13s 544ms/step - loss: 0.7150 - accuracy: 0.5284 - val_loss: 0.6579 - val_accuracy: 0.6000\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 11s 528ms/step - loss: 0.6094 - accuracy: 0.6477 - val_loss: 0.6125 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 11s 517ms/step - loss: 0.5101 - accuracy: 0.7670 - val_loss: 0.6048 - val_accuracy: 0.6000\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 11s 517ms/step - loss: 0.4018 - accuracy: 0.8580 - val_loss: 0.5762 - val_accuracy: 0.7000\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 11s 498ms/step - loss: 0.3807 - accuracy: 0.8807 - val_loss: 0.5664 - val_accuracy: 0.6500\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 11s 503ms/step - loss: 0.2964 - accuracy: 0.9261 - val_loss: 0.5243 - val_accuracy: 0.6750\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 11s 514ms/step - loss: 0.2439 - accuracy: 0.9432 - val_loss: 0.5144 - val_accuracy: 0.6750\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 11s 523ms/step - loss: 0.2124 - accuracy: 0.9659 - val_loss: 0.4674 - val_accuracy: 0.7250\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 11s 493ms/step - loss: 0.1637 - accuracy: 0.9886 - val_loss: 0.5304 - val_accuracy: 0.6750\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 11s 507ms/step - loss: 0.1474 - accuracy: 0.9886 - val_loss: 0.5149 - val_accuracy: 0.7250\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 12s 539ms/step - loss: 0.1236 - accuracy: 0.9830 - val_loss: 0.4877 - val_accuracy: 0.7750\n",
      "22/22 [==============================] - 9s 389ms/step - loss: 0.1431 - accuracy: 1.0000\n",
      "6/6 [==============================] - 2s 337ms/step - loss: 0.5196 - accuracy: 0.6818\n",
      "10/10 [==============================] - 5s 470ms/step - loss: 0.5452 - accuracy: 0.6750\n",
      "\n",
      "===== Fine-tuning VGG16 - Epochs: 20, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 3 layers out of 19 total layers in the base model.\n",
      "Training fine-tuned VGG16 with epochs=20, batch_size=16\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.7888 - accuracy: 0.5170 - val_loss: 0.6439 - val_accuracy: 0.5625\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.7033 - accuracy: 0.5909 - val_loss: 0.6648 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.6099 - accuracy: 0.6477 - val_loss: 0.6496 - val_accuracy: 0.5312\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.5357 - accuracy: 0.6818 - val_loss: 0.6200 - val_accuracy: 0.6562\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4487 - accuracy: 0.7955 - val_loss: 0.5697 - val_accuracy: 0.6250\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4015 - accuracy: 0.8409 - val_loss: 0.5908 - val_accuracy: 0.6250\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 11s 975ms/step - loss: 0.3791 - accuracy: 0.8693 - val_loss: 0.5749 - val_accuracy: 0.6250\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3386 - accuracy: 0.8864 - val_loss: 0.5302 - val_accuracy: 0.6250\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.2845 - accuracy: 0.9091 - val_loss: 0.5302 - val_accuracy: 0.7188\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2728 - accuracy: 0.9318 - val_loss: 0.5121 - val_accuracy: 0.6250\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2090 - accuracy: 0.9659 - val_loss: 0.4876 - val_accuracy: 0.7188\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 11s 998ms/step - loss: 0.1869 - accuracy: 0.9659 - val_loss: 0.5820 - val_accuracy: 0.5938\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1531 - accuracy: 0.9716 - val_loss: 0.5108 - val_accuracy: 0.6875\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1464 - accuracy: 0.9773 - val_loss: 0.5164 - val_accuracy: 0.7188\n",
      "11/11 [==============================] - 9s 782ms/step - loss: 0.1659 - accuracy: 0.9830\n",
      "3/3 [==============================] - 2s 490ms/step - loss: 0.5131 - accuracy: 0.6818\n",
      "5/5 [==============================] - 5s 925ms/step - loss: 0.5614 - accuracy: 0.7375\n",
      "\n",
      "===== Fine-tuning VGG16 - Epochs: 40, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 3 layers out of 19 total layers in the base model.\n",
      "Training fine-tuned VGG16 with epochs=40, batch_size=8\n",
      "Epoch 1/40\n",
      "22/22 [==============================] - 13s 539ms/step - loss: 0.7760 - accuracy: 0.4943 - val_loss: 0.6847 - val_accuracy: 0.5250\n",
      "Epoch 2/40\n",
      "22/22 [==============================] - 12s 529ms/step - loss: 0.6643 - accuracy: 0.6364 - val_loss: 0.6630 - val_accuracy: 0.6000\n",
      "Epoch 3/40\n",
      "22/22 [==============================] - 11s 516ms/step - loss: 0.5626 - accuracy: 0.6761 - val_loss: 0.6283 - val_accuracy: 0.6000\n",
      "Epoch 4/40\n",
      "22/22 [==============================] - 11s 532ms/step - loss: 0.5288 - accuracy: 0.7273 - val_loss: 0.6088 - val_accuracy: 0.6500\n",
      "Epoch 5/40\n",
      "22/22 [==============================] - 12s 525ms/step - loss: 0.4225 - accuracy: 0.7898 - val_loss: 0.5721 - val_accuracy: 0.7000\n",
      "Epoch 6/40\n",
      "22/22 [==============================] - 12s 531ms/step - loss: 0.3516 - accuracy: 0.8466 - val_loss: 0.5499 - val_accuracy: 0.7500\n",
      "Epoch 7/40\n",
      "22/22 [==============================] - 11s 508ms/step - loss: 0.3304 - accuracy: 0.8977 - val_loss: 0.5430 - val_accuracy: 0.6750\n",
      "Epoch 8/40\n",
      "22/22 [==============================] - 11s 505ms/step - loss: 0.2404 - accuracy: 0.9489 - val_loss: 0.5180 - val_accuracy: 0.7250\n",
      "Epoch 9/40\n",
      "22/22 [==============================] - 11s 506ms/step - loss: 0.2019 - accuracy: 0.9716 - val_loss: 0.5216 - val_accuracy: 0.7500\n",
      "Epoch 10/40\n",
      "22/22 [==============================] - 11s 504ms/step - loss: 0.1597 - accuracy: 0.9716 - val_loss: 0.4980 - val_accuracy: 0.7000\n",
      "Epoch 11/40\n",
      "22/22 [==============================] - 11s 520ms/step - loss: 0.1408 - accuracy: 0.9830 - val_loss: 0.4672 - val_accuracy: 0.8000\n",
      "Epoch 12/40\n",
      "22/22 [==============================] - 11s 514ms/step - loss: 0.1025 - accuracy: 0.9943 - val_loss: 0.4717 - val_accuracy: 0.7500\n",
      "Epoch 13/40\n",
      "22/22 [==============================] - 11s 491ms/step - loss: 0.0871 - accuracy: 0.9943 - val_loss: 0.4810 - val_accuracy: 0.7750\n",
      "Epoch 14/40\n",
      "22/22 [==============================] - 11s 505ms/step - loss: 0.0920 - accuracy: 0.9886 - val_loss: 0.5131 - val_accuracy: 0.7000\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 0.0979 - accuracy: 1.0000\n",
      "6/6 [==============================] - 2s 364ms/step - loss: 0.4808 - accuracy: 0.7500\n",
      "10/10 [==============================] - 5s 467ms/step - loss: 0.5034 - accuracy: 0.7500\n",
      "\n",
      "===== Fine-tuning VGG16 - Epochs: 40, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 3 layers out of 19 total layers in the base model.\n",
      "Training fine-tuned VGG16 with epochs=40, batch_size=16\n",
      "Epoch 1/40\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.7692 - accuracy: 0.5682 - val_loss: 0.6719 - val_accuracy: 0.5000\n",
      "Epoch 2/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.7177 - accuracy: 0.5511 - val_loss: 0.6648 - val_accuracy: 0.5312\n",
      "Epoch 3/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.6266 - accuracy: 0.6591 - val_loss: 0.6411 - val_accuracy: 0.7500\n",
      "Epoch 4/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5641 - accuracy: 0.7216 - val_loss: 0.6095 - val_accuracy: 0.6562\n",
      "Epoch 5/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4363 - accuracy: 0.8239 - val_loss: 0.6307 - val_accuracy: 0.6875\n",
      "Epoch 6/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4452 - accuracy: 0.8068 - val_loss: 0.6005 - val_accuracy: 0.6250\n",
      "Epoch 7/40\n",
      "11/11 [==============================] - 11s 974ms/step - loss: 0.3976 - accuracy: 0.8693 - val_loss: 0.6160 - val_accuracy: 0.6875\n",
      "Epoch 8/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3103 - accuracy: 0.9148 - val_loss: 0.5433 - val_accuracy: 0.7188\n",
      "Epoch 9/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2525 - accuracy: 0.9489 - val_loss: 0.5121 - val_accuracy: 0.7812\n",
      "Epoch 10/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2524 - accuracy: 0.9489 - val_loss: 0.5674 - val_accuracy: 0.6875\n",
      "Epoch 11/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2159 - accuracy: 0.9602 - val_loss: 0.4773 - val_accuracy: 0.7812\n",
      "Epoch 12/40\n",
      "11/11 [==============================] - 11s 995ms/step - loss: 0.2030 - accuracy: 0.9545 - val_loss: 0.5335 - val_accuracy: 0.7188\n",
      "Epoch 13/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1890 - accuracy: 0.9432 - val_loss: 0.5329 - val_accuracy: 0.6562\n",
      "Epoch 14/40\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1656 - accuracy: 0.9773 - val_loss: 0.5166 - val_accuracy: 0.7188\n",
      "11/11 [==============================] - 9s 777ms/step - loss: 0.1720 - accuracy: 0.9943\n",
      "3/3 [==============================] - 2s 788ms/step - loss: 0.5309 - accuracy: 0.7045\n",
      "5/5 [==============================] - 5s 970ms/step - loss: 0.5650 - accuracy: 0.7125\n",
      "\n",
      "===== Fine-tuning VGG16 - Epochs: 60, Batch Size: 8 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 3 layers out of 19 total layers in the base model.\n",
      "Training fine-tuned VGG16 with epochs=60, batch_size=8\n",
      "Epoch 1/60\n",
      "22/22 [==============================] - 13s 530ms/step - loss: 0.7762 - accuracy: 0.5000 - val_loss: 0.6773 - val_accuracy: 0.6750\n",
      "Epoch 2/60\n",
      "22/22 [==============================] - 11s 527ms/step - loss: 0.6365 - accuracy: 0.6136 - val_loss: 0.6491 - val_accuracy: 0.7000\n",
      "Epoch 3/60\n",
      "22/22 [==============================] - 11s 509ms/step - loss: 0.5304 - accuracy: 0.7727 - val_loss: 0.6035 - val_accuracy: 0.7000\n",
      "Epoch 4/60\n",
      "22/22 [==============================] - 11s 530ms/step - loss: 0.4039 - accuracy: 0.8750 - val_loss: 0.5967 - val_accuracy: 0.7000\n",
      "Epoch 5/60\n",
      "22/22 [==============================] - 11s 509ms/step - loss: 0.3943 - accuracy: 0.8295 - val_loss: 0.5840 - val_accuracy: 0.6500\n",
      "Epoch 6/60\n",
      "22/22 [==============================] - 12s 517ms/step - loss: 0.3228 - accuracy: 0.9148 - val_loss: 0.5771 - val_accuracy: 0.7250\n",
      "Epoch 7/60\n",
      "22/22 [==============================] - 12s 531ms/step - loss: 0.2461 - accuracy: 0.9773 - val_loss: 0.5741 - val_accuracy: 0.7750\n",
      "Epoch 8/60\n",
      "22/22 [==============================] - 11s 515ms/step - loss: 0.2098 - accuracy: 0.9659 - val_loss: 0.5512 - val_accuracy: 0.8000\n",
      "Epoch 9/60\n",
      "22/22 [==============================] - 11s 494ms/step - loss: 0.1982 - accuracy: 0.9716 - val_loss: 0.5439 - val_accuracy: 0.7250\n",
      "Epoch 10/60\n",
      "22/22 [==============================] - 11s 515ms/step - loss: 0.1625 - accuracy: 0.9659 - val_loss: 0.5038 - val_accuracy: 0.7750\n",
      "Epoch 11/60\n",
      "22/22 [==============================] - 11s 508ms/step - loss: 0.1265 - accuracy: 0.9886 - val_loss: 0.5327 - val_accuracy: 0.7750\n",
      "Epoch 12/60\n",
      "22/22 [==============================] - 11s 501ms/step - loss: 0.1104 - accuracy: 0.9773 - val_loss: 0.4998 - val_accuracy: 0.8000\n",
      "Epoch 13/60\n",
      "22/22 [==============================] - 11s 506ms/step - loss: 0.0851 - accuracy: 1.0000 - val_loss: 0.5032 - val_accuracy: 0.8000\n",
      "Epoch 14/60\n",
      "22/22 [==============================] - 11s 505ms/step - loss: 0.0859 - accuracy: 1.0000 - val_loss: 0.5131 - val_accuracy: 0.8000\n",
      "Epoch 15/60\n",
      "22/22 [==============================] - 11s 516ms/step - loss: 0.0637 - accuracy: 0.9943 - val_loss: 0.4909 - val_accuracy: 0.8000\n",
      "Epoch 16/60\n",
      "22/22 [==============================] - 11s 509ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.5087 - val_accuracy: 0.8000\n",
      "Epoch 17/60\n",
      "22/22 [==============================] - 11s 492ms/step - loss: 0.0496 - accuracy: 1.0000 - val_loss: 0.5021 - val_accuracy: 0.7750\n",
      "Epoch 18/60\n",
      "22/22 [==============================] - 11s 501ms/step - loss: 0.0404 - accuracy: 0.9943 - val_loss: 0.5168 - val_accuracy: 0.8000\n",
      "22/22 [==============================] - 9s 391ms/step - loss: 0.0447 - accuracy: 1.0000\n",
      "6/6 [==============================] - 2s 334ms/step - loss: 0.4995 - accuracy: 0.7727\n",
      "10/10 [==============================] - 5s 465ms/step - loss: 0.5258 - accuracy: 0.7750\n",
      "\n",
      "===== Fine-tuning VGG16 - Epochs: 60, Batch Size: 16 =====\n",
      "Found 176 images belonging to 2 classes.\n",
      "Found 44 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Fine-tuning model: Unfrozen 3 layers out of 19 total layers in the base model.\n",
      "Training fine-tuned VGG16 with epochs=60, batch_size=16\n",
      "Epoch 1/60\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.7605 - accuracy: 0.5114 - val_loss: 0.6697 - val_accuracy: 0.6250\n",
      "Epoch 2/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.6746 - accuracy: 0.6193 - val_loss: 0.6604 - val_accuracy: 0.6875\n",
      "Epoch 3/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5682 - accuracy: 0.7500 - val_loss: 0.6513 - val_accuracy: 0.6250\n",
      "Epoch 4/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.5345 - accuracy: 0.7216 - val_loss: 0.6363 - val_accuracy: 0.5625\n",
      "Epoch 5/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4417 - accuracy: 0.8239 - val_loss: 0.6010 - val_accuracy: 0.6250\n",
      "Epoch 6/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.4141 - accuracy: 0.8693 - val_loss: 0.5847 - val_accuracy: 0.6562\n",
      "Epoch 7/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.3562 - accuracy: 0.8750 - val_loss: 0.5164 - val_accuracy: 0.6250\n",
      "Epoch 8/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2917 - accuracy: 0.9205 - val_loss: 0.5516 - val_accuracy: 0.7500\n",
      "Epoch 9/60\n",
      "11/11 [==============================] - 11s 984ms/step - loss: 0.2700 - accuracy: 0.9261 - val_loss: 0.5651 - val_accuracy: 0.6250\n",
      "Epoch 10/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2238 - accuracy: 0.9659 - val_loss: 0.5054 - val_accuracy: 0.7500\n",
      "Epoch 11/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.2035 - accuracy: 0.9659 - val_loss: 0.5109 - val_accuracy: 0.7500\n",
      "Epoch 12/60\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.1732 - accuracy: 0.9830 - val_loss: 0.4920 - val_accuracy: 0.7812\n",
      "Epoch 13/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1546 - accuracy: 0.9716 - val_loss: 0.5078 - val_accuracy: 0.6875\n",
      "Epoch 14/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1475 - accuracy: 0.9830 - val_loss: 0.4972 - val_accuracy: 0.7812\n",
      "Epoch 15/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.1237 - accuracy: 0.9886 - val_loss: 0.4472 - val_accuracy: 0.8125\n",
      "Epoch 16/60\n",
      "11/11 [==============================] - 11s 987ms/step - loss: 0.1102 - accuracy: 0.9943 - val_loss: 0.5508 - val_accuracy: 0.6562\n",
      "Epoch 17/60\n",
      "11/11 [==============================] - 11s 1s/step - loss: 0.0928 - accuracy: 0.9943 - val_loss: 0.4645 - val_accuracy: 0.8125\n",
      "Epoch 18/60\n",
      "11/11 [==============================] - 11s 988ms/step - loss: 0.0834 - accuracy: 1.0000 - val_loss: 0.5288 - val_accuracy: 0.7500\n",
      "11/11 [==============================] - 9s 776ms/step - loss: 0.0874 - accuracy: 1.0000\n",
      "3/3 [==============================] - 2s 751ms/step - loss: 0.4762 - accuracy: 0.7955\n",
      "5/5 [==============================] - 5s 947ms/step - loss: 0.5511 - accuracy: 0.7125\n",
      "\n",
      "All processes completed successfully!\n",
      "Results saved to: C:/Users/user/Desktop/MLWORK/VGG16\n"
     ]
    }
   ],
   "source": [
    "# 主函數\n",
    "def main():\n",
    "    # 創建CSV結果文件\n",
    "    csv_files = create_result_csv_files()\n",
    "    \n",
    "    model_name = \"VGG16\"\n",
    "    all_results = {}  # 儲存所有結果以供後續比較\n",
    "    \n",
    "    # 第1階段：訓練和評估基礎模型\n",
    "    print(\"======== Stage 1: Training and Evaluating Base Models ========\")\n",
    "    \n",
    "    for epochs in epochs_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            print(f\"\\n===== Training Base {model_name} - Epochs: {epochs}, Batch Size: {batch_size} =====\")\n",
    "            \n",
    "            # 創建資料生成器\n",
    "            train_gen, val_gen, test_gen = create_data_generators(batch_size)\n",
    "            \n",
    "            # 創建基礎模型\n",
    "            base_model = create_vgg_model()\n",
    "            \n",
    "            # 訓練和評估基礎模型\n",
    "            base_results = train_and_evaluate_model(\n",
    "                base_model, train_gen, val_gen, test_gen,\n",
    "                model_name, epochs, batch_size, is_fine_tuned=False\n",
    "            )\n",
    "            \n",
    "            # 保存結果到CSV\n",
    "            save_results_to_csv(\n",
    "                base_results, csv_files['base'], \n",
    "                model_name, epochs, is_fine_tuned=False\n",
    "            )\n",
    "            \n",
    "            # 保存模型結果以供後續比較\n",
    "            result_key = f\"{model_name}_e{epochs}_b{batch_size}\"\n",
    "            all_results[result_key] = {'base': base_results}\n",
    "            \n",
    "            # 清理內存\n",
    "            tf.keras.backend.clear_session()\n",
    "    \n",
    "    # 第2階段：微調模型\n",
    "    print(\"\\n======== Stage 2: Fine-tuning Models ========\")\n",
    "    \n",
    "    for epochs in epochs_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            print(f\"\\n===== Fine-tuning {model_name} - Epochs: {epochs}, Batch Size: {batch_size} =====\")\n",
    "            \n",
    "            # 創建資料生成器 (重新創建是為了確保數據的一致性)\n",
    "            train_gen, val_gen, test_gen = create_data_generators(batch_size)\n",
    "            \n",
    "            # 創建模型\n",
    "            tuned_model = create_vgg_model()\n",
    "            \n",
    "            # 微調模型\n",
    "            tuned_model = fine_tune_model(tuned_model)\n",
    "            \n",
    "            # 需要重新編譯模型以應用更低的學習率\n",
    "            tuned_model.compile(\n",
    "                optimizer=Adam(learning_rate=1e-5),  # 更低的學習率用於微調\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # 訓練和評估微調後的模型\n",
    "            tuned_results = train_and_evaluate_model(\n",
    "                tuned_model, train_gen, val_gen, test_gen,\n",
    "                model_name, epochs, batch_size, is_fine_tuned=True\n",
    "            )\n",
    "            \n",
    "            # 保存結果到CSV\n",
    "            save_results_to_csv(\n",
    "                tuned_results, csv_files['tuned'], \n",
    "                model_name, epochs, is_fine_tuned=True\n",
    "            )\n",
    "            \n",
    "            # 添加到結果字典\n",
    "            result_key = f\"{model_name}_e{epochs}_b{batch_size}\"\n",
    "            if result_key in all_results:\n",
    "                all_results[result_key]['tuned'] = tuned_results\n",
    "                \n",
    "                # 保存比較結果\n",
    "                base_results = all_results[result_key]['base']\n",
    "                save_comparison_to_csv(\n",
    "                    base_results, tuned_results, csv_files['comparison'],\n",
    "                    model_name, epochs\n",
    "                )\n",
    "            \n",
    "            # 清理內存\n",
    "            tf.keras.backend.clear_session()\n",
    "    \n",
    "    print(\"\\nAll processes completed successfully!\")\n",
    "    print(f\"Results saved to: {results_dir}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b8607-a990-4b9b-9454-f24231c96fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c10db7-a6cd-4bc7-9b05-6bd38a574677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
